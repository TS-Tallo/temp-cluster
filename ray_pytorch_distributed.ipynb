{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Ray with PyTorch Distributed Training\n",
    "\n",
    "This notebook demonstrates how to use Ray with PyTorch for distributed model training and inference."
   ],
   "id": "1909936143bed7c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import numpy as np\n",
    "\n",
    "# Install Ray if not already installed\n",
    "try:\n",
    "    import ray\n",
    "except ImportError:\n",
    "    !pip install ray[train]\n",
    "    import ray\n",
    "\n",
    "from ray import train\n",
    "from ray.train import Trainer, TrainingCallback\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.air.config import ScalingConfig\n",
    "\n",
    "# Initialize Ray\n",
    "ray.init(address=\"auto\", namespace=\"ray_pytorch_example\")\n",
    "print(\"Ray initialized. Available resources:\", ray.available"
   ],
   "id": "c62a7b7c683d72a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_size=10, hidden_size=50, output_size=1):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ],
   "id": "39da6d8feb71e69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_func(config):\n",
    "    # Get the rank of the current process\n",
    "    rank = train.get_context().get_world_rank()\n",
    "    world_size = train.get_context().get_world_size()\n",
    "\n",
    "    # Set up the distributed process group\n",
    "    dist.init_process_group(backend=\"nccl\" if torch.cuda.is_available() else \"gloo\")\n",
    "\n",
    "    # Create a device to train on\n",
    "    device = torch.device(f\"cuda:{train.get_context().get_local_rank()}\"\n",
    "                         if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Create fake dataset (in real applications, you'd use a proper DataLoader)\n",
    "    input_size = config[\"input_size\"]\n",
    "    dataset_size = config[\"dataset_size\"]\n",
    "\n",
    "    # Generate synthetic data\n",
    "    X = torch.randn(dataset_size, input_size)\n",
    "    y = torch.randn(dataset_size, 1)\n",
    "\n",
    "    # Create model and move to device\n",
    "    model = SimpleModel(input_size=input_size,\n",
    "                       hidden_size=config[\"hidden_size\"],\n",
    "                       output_size=config[\"output_size\"])\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Wrap model with DDP\n",
    "    model = DDP(model, device_ids=[train.get_context().get_local_rank()]\n",
    "               if torch.cuda.is_available() else None)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "    # Training loop\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        total_loss = 0\n",
    "        for i in range(0, dataset_size, batch_size):\n",
    "            # Get batch\n",
    "            indices = slice(i, min(i + batch_size, dataset_size))\n",
    "            inputs = X[indices].to(device)\n",
    "            targets = y[indices].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Report metrics\n",
    "        if rank == 0:  # Only report from the first worker\n",
    "            avg_loss = total_loss / (dataset_size // batch_size)\n",
    "            train.report({\"loss\": avg_loss})\n",
    "            print(f\"Epoch {epoch+1}/{config['num_epochs']}, Loss: {avg_loss:.4f}\")"
   ],
   "id": "3334a9fe3a1dcc9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define training configuration\n",
    "train_config = {\n",
    "    \"input_size\": 10,\n",
    "    \"hidden_size\": 50,\n",
    "    \"output_size\": 1,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"num_epochs\": 5,\n",
    "    \"dataset_size\": 1000\n",
    "}\n",
    "\n",
    "# Configure the Ray Trainer\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=2,  # Adjust based on available resources\n",
    "    use_gpu=torch.cuda.is_available(),\n",
    "    resources_per_worker={\"CPU\": 1, \"GPU\": 1 if torch.cuda.is_available() else 0}\n",
    ")\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_func,\n",
    "    train_loop_config=train_config,\n",
    "    scaling_config=scaling_config,\n",
    ")\n",
    "\n",
    "# Run the training\n",
    "result = trainer.fit()\n",
    "print(\"Training complete!\")\n",
    "print(f\"Final metrics: {result.metrics}\")"
   ],
   "id": "4dd50c50052828db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@ray.remote(num_gpus=1 if torch.cuda.is_available() else 0)\n",
    "class ModelWorker:\n",
    "    def __init__(self, model_config):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = SimpleModel(\n",
    "            input_size=model_config[\"input_size\"],\n",
    "            hidden_size=model_config[\"hidden_size\"],\n",
    "            output_size=model_config[\"output_size\"]\n",
    "        ).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        # Convert inputs to tensor if needed\n",
    "        if not isinstance(inputs, torch.Tensor):\n",
    "            inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "\n",
    "        # Move to device and ensure correct shape\n",
    "        inputs = inputs.to(self.device)\n",
    "        if inputs.dim() == 1:\n",
    "            inputs = inputs.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(inputs)\n",
    "\n",
    "        return outputs.cpu().numpy()\n",
    "\n",
    "# Example usage\n",
    "def run_distributed_inference():\n",
    "    # Initialize worker actors\n",
    "    num_workers = min(2, torch.cuda.device_count() if torch.cuda.is_available() else 1)\n",
    "    model_config = {\n",
    "        \"input_size\": 10,\n",
    "        \"hidden_size\": 50,\n",
    "        \"output_size\": 1\n",
    "    }\n",
    "\n",
    "    workers = [ModelWorker.remote(model_config) for _ in range(num_workers)]\n",
    "\n",
    "    # Generate some random input data for testing\n",
    "    batch_size = 10\n",
    "    test_inputs = [torch.randn(model_config[\"input_size\"]) for _ in range(batch_size)]\n",
    "\n",
    "    # Distribute inference requests across workers (round-robin)\n",
    "    futures = []\n",
    "    for i, data in enumerate(test_inputs):\n",
    "        worker_idx = i % len(workers)\n",
    "        futures.append(workers[worker_idx].predict.remote(data))\n",
    "\n",
    "    # Gather results\n",
    "    results = ray.get(futures)\n",
    "    return results\n",
    "\n",
    "# Run the distributed inference\n",
    "inference_results = run_distributed_inference()\n",
    "print(f\"Inference complete. Results shape: {np.array(inference_results).shape}\")"
   ],
   "id": "9c4336cb68c515e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Clean up Ray resources\n",
    "ray.shutdown()\n",
    "print(\"Ray resources released.\")"
   ],
   "id": "9eea35885c989b9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
